\section{Introduction}

Distributional approaches to representing the semantics of natural language
are an established and important component in the computational
linguist's toolbox. In these approaches, words are typically
represented as vectors, however we still lack a comprehensive theory
of natural language meaning that is vector based.

Part of the problem is arguably that we don't know how to relate
distributional semantics to logical semantics. Recent work has looked
at how vector based representations of meaning can be composed in a
similar way to how fragments of logic are composed in Montague
semantics.

However there is another problem that has until now not received a lot
of attention. Researchers in distributional semantics typically focus
on similarity, whilst logical semantics places an emphasis on
equivalence, contradiction and entailment. Whilst similarity is
sufficient for many tasks in natural language processing, tasks such
as textual entailment recognition, summarisation and ontology learning
deal with entailment. Similarity measures are not sufficient to
represent entailment: they are generally symmetric, whilst entailment
is antisymmetric and transitive.

In this paper we examine the proposition of \newcite{Clarke:07} that
entailment can be described by a partial ordering on the vector
space. Whilst many vector spaces can be considered to be partially
ordered vector spaces merely by the existence of a distinguished
basis, in this paper we investigate the possibility of learning a new
partial ordering directly from data. We describe an algorithm which
does this and demonstrate its effectiveness on some simple datasets.

We also describe how this technique could potentially be applied to
three different problems: recognising textual entailment, ontology
learning, and compositionality in vector space semantics. As this is
preliminary work, we leave the implementation of these proposals to
further work.
