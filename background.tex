\section{Background}
\label{sec:background}

In this section, we  briefly discuss three distinct notions of entailment that have played a role
in natural language semantics, then go on to consider in more detail what has been referred to as
distributional entailment.

\subsection{Entailment}

There are at least three notions of entailment described in the
literature. The oldest and best defined is \textbf{logical entailment}, which
is (in general) a transitive and antisymmetric relation (i.e.~a
preorder) on a formal language (that of the logic in question).

\textbf{Lexical entailment}~\cite{Geffet:05} is that described by taxonomies, 
such as the IS-A
or hypernymy relation in WordNet. For example, a \emph{cat} is an
\emph{animal}, so \emph{cat} lexically entails \emph{animal}. 
Like logical entailment, this relation is normally
assumed to be a preorder, and often it is also assumed to be a partial
order.

Finally, \textbf{textual entailment}~\cite{Dagan:05} is a recent 
innovation intended to
characterise a problem that lies at the heart of many of the tasks 
that arise in natural language processing applications, including
document summarisation, information retrieval and machine
translation. A sentence $T$ (the ``text'') is said to entail a
sentence $H$ (the ``hypothesis'') if a reader will generally infer
that $H$ is true given that $T$ is true. This is clearly a more fuzzy
relation than logical entailment, leading to disagreement among readers
as to whether this condition holds in certain cases.

\subsection{Distributional Entailment}

\newcite{Weeds:04} proposed the notion of \textbf{distributional
  generality}, that words with more general meanings will tend to
occur in a wider range of contexts. \newcite{Clarke:07} formalised
this idea using a partially ordered vector space.

\begin{definition}[Partially ordered vector space]
  A partially ordered vector space $V$ is a real vector space together
  with a partial ordering $\le$ such that:\\[5pt]
  \indent if $\mathbf{u}\le\mathbf{v}$ then $\mathbf{u} + \mathbf{w} \le\mathbf{v} + \mathbf{w}$\\[5pt]
  \indent if $\mathbf{u} \le\mathbf{v}$ then $\alpha \mathbf{u} \le \alpha \mathbf{v}$
  \vspace{0.1cm}\\[5pt]
  for all $\mathbf{u},\mathbf{v},\mathbf{w} \in V$, and for all $\alpha \ge 0$. 
  
  Such a partial
  ordering is called a \textbf{vector space order} on $V$. An element
  $u$ of $V$ satisfying $u \ge 0$ is called a \textbf{positive
    element}; the set of all positive elements of $V$ is denoted
  $V^+$. If $\le$ defines a lattice on $V$ then the space is called a
  \textbf{vector lattice} or \textbf{Riesz space}.
\end{definition}

\begin{example}[Lattice operations on $\R^n$]
  \label{example:finite}
  A vector lattice captures many properties that are inherent in real
  vector spaces when there is a \emph{distinguished basis}. In $\R^n$,
  given a specific basis, we can write two vectors $\mathbf{u}$ and $\mathbf{v}$ as
  sequences of numbers: $\mathbf{u} = (u_1,u_2,\ldots u_n)$ and $\mathbf{v} =
  (v_1,v_2,\ldots v_n)$. This allows us to define the lattice
  operations of meet $\land$ and join $\lor$ as
\[\begin{array}{l}
\mathbf{u}\land \mathbf{v} =\\
(\min(u_1,v_1),\min(u_2,v_2),\ldots \min(u_n,v_n))\\[5pt]
\mathbf{u}\lor \mathbf{v} = \\
(\max(u_1,v_1),\max(u_2,v_2),\ldots \max(u_n,v_n))
\end{array}\]
These are the component-wise minimum and maximum, respectively. The partial
ordering is then given by $\mathbf{u} \le\mathbf{v}$ if and only if $\mathbf{u} \land \mathbf{v} = u$, or
equivalently $u_i \le v_i$ for all $i$.
\end{example}

In computational linguistics, nearly all vector spaces under
consideration have a basis, and thus have an implicit lattice ordering
as defined above. This ordering is interpreted as a ``distributional
entailment'' relation, so a term $x$ distributionally entails a term
$y$ if $\hat{x} \le \hat{y}$, where $\hat{x}$ and $\hat{y}$ are the
context vectors for terms $x$ and $y$. The hypothesis implicit in the
work of \newcite{Clarke:07} is that there exist vector representations
for terms and phrases such that distributional entailment correlates
or coincides with other notions of entailment. In this paper we
investigate this hypothesis with respect to lexical
entailment. However, we also suggest that instead of considering the
implicitly defined lattice ordering, it may be possible to learn a new
ordering on the vector space by making distributional entailment
coincide with other notions of entailment, using machine learning
techniques.

