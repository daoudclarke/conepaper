
\section{Related Work}

\newcite{Weeds:04}, who introduced the notion of distributional
generality, considered the simpler task of determining the entailment
direction, given two words for which it is known that entailment
exists, but the direction of entailment is not know. They evaluated
two unsupervised ways of doing this on hyponym-hypernym pairs
extracted from WordNet. Firstly, they found that the frequency of the
term predicted the entailment direction 70\% of the time, with the
more general term being the more frequent. Secondly, they looked at
the precision and recall of the features associated with two terms,
where the features of one term are used to predict the occurence of
the feature with the other term; this gives a measure of which term
occurs in a wider range of contexts. They found this predicted the
correct entailment direction 71\% of the time. Along this strain of
unsupervised approaches, there have been a number of investigations
into directional or asymmetric distributional similarity measures and
their application
\cite{Geffet:05,Bhagat:07,Szpektor:08,Clarke:09,Kotlerman:10}.

The Stanford WordNet project \cite{Snow:04} expands the WordNet
taxonomy by analysing large corpora to find patterns that are
indicative of hyponymy. For example, the pattern ``$\mathit{NP}_X$ and
other $\mathit{NP}_Y$'' is an indication that $\mathit{NP}_X$ is a
$\mathit{NP}_Y$, i.e.~that $\mathit{NP}_X$ is a hyponym of
$\mathit{NP}_Y$. They use machine learning to identify other such
patterns from known hyponym-hypernym pairs, and then use these
patterns to find new relations in the corpus. They enforce the
transitivity relation of the taxonomy by only searching over valid
taxonomies, and evaluating the likelihood of each taxonomy given the
available evidence \cite{Snow:06}. A cone classifier would obviate the
need for this search, since only valid taxonomies would be produced by
the classifier.

\newcite{Kobayashi:08} propose a method of classification using
cones. In their method, the cone is generated by the positive class,
and classification is performed by projecting a given vector onto the
cone and measuring the angle between the original and projected
vectors. An instance whose angle is less than a learnt threshold is
assumed to be positive. Their approach works since their goal is
different to ours: they are interested only in classification, not
learning a partial ordering. Moreover, since they use the generating
cone, it is not clear how sensitive their technique is to noise in the
training data, whereas our approach explicitly accounts for this.
