%Bismillahi-r-Rahmani-r-Rahim
\documentclass{article}
\usepackage{acl2013}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{graphicx}
\usepackage{csvsimple}

\usepackage{amsthm}
\usepackage{amssymb}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}

\newcommand{\R}{\mathbb{R}}

\title{Experiments with Learning Cones}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}

Distributional approaches to representing meaning in natural language
are an established and important component in the computational
linguist's toolbox. In these approaches, words are typically
represented as vectors, however we still lack a comprehensive theory
of natural language meaning that is vector based.

Part of the problem is arguably that we don't know how to relate
distributional semantics to logical semantics. Recent work has looked
at how vector based representations of meaning can be composed in a
similar way to how fragments of logic are composed in Montague
semantics.

However there is another problem that has until now not received a lot
of attention. Researchers in distributional semantics typically focus
on similarity, whilst logical semantics places an emphasis on
equivalence, contradiction and entailment. Whilst similarity is
sufficient for many tasks in natural language processing, tasks such
as textual entailment recognition, summarisation and ontology learning
deal with entailment. Similarity measures are not sufficient to
represent entailment: they are generally symmetric, whilst entailment
is antisymmetric and transitive.

In this paper we examine the proposition of \newcite{Clarke:07} that
entailment can be described by a partial ordering on the vector
space. Whilst many vector spaces can be considered to be partially
ordered vector spaces merely by the existence of a distinguished
basis, in this paper we investigate the possibility of learning a new
partial ordering directly from data. We describe an algorithm which
does this and demonstrate its effectiveness on some simple datasets.

We also describe how this technique could potentially be applied to
three different problems: recognising textual entailment, ontology
learning, and compositionality in vector space semantics. As this is
preliminary work, we leave the implementation of these proposals to
further work.

\section{Background}

\subsection{Entailment}

There are at least three notions of entailment described in the
literature. The oldest and best defined is logical entailment, which
is (in general) a transitive and antisymmetric relation (i.e.~a
preorder) on a formal language (that of the logic in question).

Lexical entailment is that described by taxonomies, such as the IS-A
or hypernymy relation in WordNet, for example, a \emph{cat} is an
\emph{animal}. Like logical entailment, this relation is normally
assumed to be a preorder, and often it is also assumed to be a partial
order.

Finally, textual entailment is a recent innovation intended to
generalise several tasks in natural language processing, including
document summarisation, information retrieval and machine
translation. A sentence $T$ (the ``text'') is said to entail a
sentence $H$ (the ``hypothesis'') if a reader will generally infer
that $H$ is true given that $T$ is true. This is arguably a fuzzy
relation since different readers will often disagree.

\subsection{Distributional Entailment}

\newcite{Weeds:04} proposed a notion of distributional generality, that
words with more general meanings will tend to occur in a wider range
of contexts. \newcite{Clarke:07} formalised this idea using a partially
ordered vector space.

\begin{definition}[Partially ordered vector space]
  A partially ordered vector space $V$ is a real vector space together
  with a partial ordering $\le$ such that:
  \vspace{0.1cm}\\
  \indent if $x \le y$ then $x + z \le y + z$\\
  \indent if $x \le y$ then $\alpha x \le \alpha y$
  \vspace{0.1cm}\\
  for all $x,y,z \in V$, and for all $\alpha \ge 0$. Such a partial
  ordering is called a \textbf{vector space order} on $V$. An element
  $u$ of $V$ satisfying $u \ge 0$ is called a \textbf{positive
    element}; the set of all positive elements of $V$ is denoted
  $V^+$. If $\le$ defines a lattice on $V$ then the space is called a
  \textbf{vector lattice} or \textbf{Riesz space}.
\end{definition}

\begin{example}[Lattice operations on $\R^n$]
  \label{example:finite}
  A vector lattice captures many properties that are inherent in real
  vector spaces when there is a \emph{distinguished basis}. In $\R^n$,
  given a specific basis, we can write two vectors $u$ and $v$ as
  sequences of numbers: $u = (u_1,u_2,\ldots u_n)$ and $v =
  (v_1,v_2,\ldots v_n)$. This allows us to define the lattice
  operations of meet $\land$ and join $\lor$ as
\begin{eqnarray*}
u\land v &=& (\min(u_1,v_1),\min(u_2,v_2),\ldots \min(u_n,v_n))\\
u\lor v &=& (\max(u_1,v_1),\max(u_2,v_2),\ldots \max(u_n,v_n))
\end{eqnarray*}
These are the component-wise minimum and maximum, respectively. The partial
ordering is then given by $u \le v$ if and only if $u \land v = u$, or
equivalently $u_i \le v_i$ for all $i$.
\end{example}

\section{Motivation}



\section{Learning Lattice Orderings}

In this section we show how lattice orderings on finite dimensional
spaces can be described as matrices, and give our algorithm for
learning such matrices from data.

\begin{proposition}[Partial Orderings Defined by Linear Operators]
  Let $A$ be a vector space and $B$ a partially ordered vector space,
  and let $M$ be a linear function from $A$ to $B$. Define an ordering
  on $A$ by $u\le v$ if and only if $Mu \le Mv$. Then $A$ is a
  partially ordered vector space under this ordering.
\end{proposition}

\begin{proof}
If $u \le v$ then $M(u + z) = Mu + Mz \le Mv + Mz = M(v +
z)$ so $u + z \le v + z$. Similarly, $M(\alpha u) = \alpha Mu
\le\alpha Mv = M(\alpha v)$ so $\alpha u \le \alpha v$ for
$\alpha \ge 0$.
\end{proof}

In particular, if we take $A = \R^n$ and $B = \R^m$ then $M$ is an $n$
by $m$ matrix. If we take the ordering for $A$ from Example
\ref{example:finite} then $u\le v$ if and only if $(Mu)_i \le (Mv)_i$
for all $i$.

\subsection{Learning Algorithm}

In this section we describe our learning algorithm.

\begin{algorithm}
\caption{Cone Learning by Gradient Descent}\label{algorithm:cone}
\begin{algorithmic}
  \Procedure{LearnCone}{$V, \mathbf{c}, d$}
  \State $M \gets$ \Call{RandMatrix}{d}
  \State $\alpha \gets \alpha_0$
  \Loop
    \State $M' \gets$ \Call{Gradient}{$V, \mathbf{c}, M$}
    \Loop
      \State $N \gets M - \alpha M'$
      \State $N' \gets$ \Call{Gradient}{$V, \mathbf{c}, N$}
      \If{$\|N'\|_1 < \|M'\|_1$}
        \State \textbf{break}
      \EndIf
      \State $\alpha \gets \alpha/2$
    \EndLoop
    \State $M \gets N$
    \State $\alpha \gets 1.2\alpha$
  \EndLoop
  \EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Experiments}

\subsection{Toy Data}

\subsection{MlData.org Datasets}

\subsection{Hypernymy Detection}

We built a dataset using distributional information of nouns together
with taxonomy information from WordNet.
\begin{enumerate}
\item We identified ``pseudo-monosemous'' nouns in WordNet by
  selecting those whose most most frequent sense accounted for more
  than 80\% of occurrences (we used the frequency information included
  with WordNet), giving us 2,351 nouns.
\item We removed the term \emph{entity} as this is indirectly related
  to every noun, and we didn't want our dataset to be biased by this
  edge case.
\item We identified the set $H$ of all pairs of terms $(t_1, t_2)$
  such that $t_2$ is an indirect hypernym of $t_1$, using the most
  frequent sense of each term. This gave us 4,794 pairs; the first ten
  are shown in Table \ref{table:pairs}.
\item We chose the same number of pairs at random from the remainder
  of possible pairs to form non-positive examples
\item For each term, we built a vector using dependency relations,
  extracted using (parser?) from (corpus?).
\item We constructed the dataset using the vector $u_2 - u_1$ where
  $u_i$ is the vector obtained for term $t_i$, and where the class was
  positive if the indirect hypernymy relation holds for the pair.
\item Reduce the dimensionality using sparse random projections.
\end{enumerate}

\begin{table}
\begin{center}
\begin{minipage}{3cm}
abandon trait\\
abortion event\\
abscess knowledge\\
absurdity nonsense\\
abyss object\\
abyss location\\
academy institution\\
academy school\\
academy group\\
academy grouping\\
\end{minipage}
\end{center}
\caption{The first ten entailment pairs obtained from WordNet.}
\label{table:pairs}
\end{table}

\begin{table}
\begin{center}
\csvautotabular{datasets.csv}
\end{center}
\caption{Datasets from mldata.org used in our experiments, with number
  of instances, features and classes in each dataset.}
\end{table}

\section{Results}

\begin{table*}
\begin{center}
\csvautotabular{results-all.csv}
\end{center}
\end{table*}


% \begin{table*}
% \begin{center}
% \csvautotabular{toy.csv}
% \end{center}
% \caption{Accuracy results on toy data for each classifier. The first
%   number in the dataset name indicates the dimensionality of the data,
%   and the second the number of vectors generating the cone used
%   to construct the data.}
% \end{table*}

% \begin{table*}
% \begin{center}
% \csvautotabular{accuracy.csv}
% \end{center}
% \caption{Accuracy of each classifier on the mldata.org datasets, with
%   estimates of error in the mean derived from three-fold
%   cross-validation.}
% \end{table*}


% \begin{table*}
% \begin{center}
% \begin{tabular}{|l|l|l|}
% \hline
% Dataset & SVM Time & Cone Time\\
% \hline
% colon-cancer & 0.267 $\pm$ 0.002 & 22.677 $\pm$ 0.728\\
% iris & 0.085 $\pm$ 0.001 & 129.210 $\pm$ 2.384\\
% mnist-original & 2716.761 $\pm$ 20.363 & 29181.849 $\pm$ 9.064\\
% sensit-vehicle-combined & 1193.599 $\pm$ 13.237 & 5157.205 $\pm$ 10.313\\
% sonar & 0.211 $\pm$ 0.009 & 105.201 $\pm$ 1.638\\
% \hline
% \end{tabular}
% \end{center}
% \caption{Training times for each classifier.}
% \end{table*}


% \begin{figure}
% \begin{center}
% \includegraphics[width=\linewidth]{dimensions.pdf}
% \end{center}
% \caption{Accuracy of each dataset with the cone classifier (y axis) as
%   the dimensionality (x axis) varies. This data is derived from the
%   parameter search done using a second cross-validation done on each
%   fold.}
% \end{figure}

% \begin{table*}
% \begin{center}
% \csvautotabular{wn-greedy.csv}
% \end{center}
% \caption{Accuracy on datasets generated from distributional
%   information and WordNet taxonomy. The last number indicates the
%   number of dimensions after reduction with random projections.}
% \end{table*}



\bibliographystyle{acl}
\bibliography{contexts}


\end{document}