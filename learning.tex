\section{Learning Partial Orderings}

In this section we show how partial orderings on finite dimensional
spaces can be described as matrices, and give our algorithm for
learning such matrices from data.

\begin{figure}
\begin{center}
\scalebox{0.7}{
\input{basis.tex}
}
\caption{The basis associated with a vector space defines an
  ordering. This shows the join of two elements in the plane with the
  ordering defined by the basis formed from the $x$ and $y$ axes. The
  shaded areas are $u + C$ and $v + C$ where $C$ is the positive cone
  defined by this ordering. }
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\scalebox{0.7}{
\input{cone.tex}
}
\end{center}
\caption{A cone $C$ defining a lattice ordering on the two dimensional
  vector space in the plane.}
\end{figure}

\begin{figure}
\begin{center}
\scalebox{0.7}{
\input{cones.tex}
}
\end{center}
\caption{The join of two elements $u$ and $v$ and the spaces $u + C$
  and $v + C$ where $C$ is the cone defining the ordering of the
  space.}
\end{figure}

A \textbf{cone} is a subset $C$ of a vector space satisfying
\begin{itemize}
\item $C + C \subseteq C$
\item $\alpha C \subseteq C$ for all $\alpha \ge 0$
\item $C \cap (-C) = \{0\}$
\end{itemize}
If $V$ is a partially ordered vector space, then the set $V^+ = \{v
\in V : v \ge 0\}$ is a cone, called the \textbf{positive
  cone}. Conversely, given any cone $C$ for a vector space $V$, we can
define a partial ordering on $V$ by $u \le v$ iff $v - u \in C$.

Given a countable set $U = \{u_1, u_2, \ldots\}$ of vectors, the cone
generated by $U$ is the set $C_U = \{v : v = \sum_i \alpha_i u_i\}$ for
some $\alpha_i \ge 0$.

\subsection{Matrices as Orderings}

\begin{proposition}[Partial Orderings Defined by Linear Operators]
  Let $A$ be a vector space and $B$ a partially ordered vector space,
  and let $M$ be a linear function from $A$ to $B$. Define an ordering
  on $A$ by $u\le v$ if and only if $Mu \le Mv$. Then $A$ is a
  partially ordered vector space under this ordering.
\end{proposition}

\begin{proof}
If $u \le v$ then $M(u + z) = Mu + Mz \le Mv + Mz = M(v +
z)$ so $u + z \le v + z$. Similarly, $M(\alpha u) = \alpha Mu
\le\alpha Mv = M(\alpha v)$ so $\alpha u \le \alpha v$ for
$\alpha \ge 0$.
\end{proof}

In particular, if we take $A = \R^n$ and $B = \R^m$ then $M$ is an $n$
by $m$ matrix. If we take the ordering for $A$ from Example
\ref{example:finite} then $u\le v$ if and only if $(Mu)_i \le (Mv)_i$
for all $i$.

\subsection{Learning Algorithm}

\begin{algorithm}
\caption{Cone Learning by Gradient Descent}\label{algorithm:cone}
\begin{algorithmic}
  \Procedure{LearnCone}{$V, \mathbf{c}, d$}
  \State $M \gets$ \Call{RandMatrix}{d}
  \State $\alpha \gets \alpha_0$
  \Loop
    \State $M' \gets$ \Call{Gradient}{$V, \mathbf{c}, M$}
    \Loop
      \State $N \gets M - \alpha M'$
      \State $N' \gets$ \Call{Gradient}{$V, \mathbf{c}, N$}
      \If{$\|N'\|_1 < \|M'\|_1$}
        \State \textbf{break}
      \EndIf
      \State $\alpha \gets \alpha/2$
      \If{$\alpha < \alpha_t$}
        \State \textbf{return} $M$
      \EndIf
    \EndLoop
    \State $M \gets N$
    \State $\alpha \gets 1.2\alpha$
  \EndLoop
  \EndProcedure
  \\
  \Procedure{Gradient}{$V, \mathbf{c}, M$}
  \State $A \gets MV$
  \State $B \gets$ \Call{Project}{$A, \mathbf{c}$}
  \State \textbf{return} $(A - B)V^T$
  \EndProcedure
  \\
  \Procedure{Project}{$U, \mathbf{c}$}
  \For{each column vector $\mathbf{u}_i$ in $U$}
    \If{$c_i = 1$}
      \State $\mathbf{u}_i \gets (\mathbf{u}_i)^+$
    \Else
      \State $m \gets \min(\mathbf{u}_i)$
      \If{$m > -0.1$}
        \State $j' \gets \argmin_j{(\mathbf{u}_i)_j}$
        \State $(\mathbf{u}_i)_j \gets -0.1$
      \EndIf
    \EndIf
  \EndFor
  \State \textbf{return} $U$
  \EndProcedure
\end{algorithmic}
\end{algorithm}

In this section we describe our learning algorithm (Algorithm
\ref{algorithm:cone}). Our approach is inspired by that of
\newcite{Hoyer:04}, who describes an approach for learning sparse
non-negative matrix factorisations based on gradient descent. Although
our goal and implementation is unique, we are able to adapt the
gradient descent technique to our ends. Our algorithm is implemented
in Python and is open sourced under the MIT license.\footnote{Location
  withheld to preserve anonymity.}

Let $\mathbf{v}$ be an instance vector and $c \in \{0,1\}$ the class value of
the instance, where 0 indicates the instance is not positive, and 1
indicates that it is positive. Our goal is to learn a matrix $M$ such
that $M\mathbf{v} \ge 0$ if and only if $c = 1$, for as many instance vectors
as possible.

Let $V$ be the matrix whose columns are the instance vectors in the
training data, and let $\mathbf{c}$ be the binary vector of
corresponding class values.

We also need to fix the parameter $d$, the dimensionality of the space
describing the partial ordering. Then $M$ is a $d$ by $n$ matrix,
where $n$ is the number of instance features, i.e.~the dimensionality
of the instance vectors $\mathbf{v}$. We initialise this to a random
matrix with values in $[-1,1]$, and then begin the loop in which the
gradient descent occurs. The gradient $M'$ is computed by projecting
$MV$ such the resulting matrix satisfies the goal of making all
positive instances in the training data positive ($\mathbf{u}^+$ is
the vector with all negative components of $\mathbf{u}$ set to zero)
and all non-positive instances are not positive. The gradient is then
estimated by multiplying the difference between $MV$ and its
projection with $V^T$.

The parameter $\alpha$ is tuned using a heuristic from the
implementation of \newcite{Hoyer:04}, and determines how far we move
in the direction of the gradient in each loop. We initialise $\alpha$
to $\alpha_0$ and stop when $\alpha$ is smaller than the threshold
$\alpha_t$.

\subsection{Avoiding Local Minima}

We found in our initial experiments, particularly with low-dimensional
data, that the gradient descent algorithm gets stuck in local
minima. To mitigate this, we do a few iterations of the gradient
descent algorithm from a number of different initial matrices, and
choose the one that works the best on the training data. We then do a
gradient descent until convergence on this best performing matrix.

\subsection{Handling Noisy Data}

We also found in testing that the algorithm could not cope well with
noisy data; in particular if a data point is marked as positive but is
actually in the negation of the cone, this prevents the algorithm from
finding the optimal solution. To get around this, we relaxed the
projection algorithm so that it only affects those that are already
closest to being correct. This effectively gives us automatic outlier
detection, where the proportion of outliers that should not be
adjusted in the projection is specified as a parameter.

